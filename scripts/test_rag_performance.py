#!/usr/bin/env python3
"""
RAG Performance Test Script

RAG ÏãúÏä§ÌÖúÏùò Í≤ÄÏÉâ Ï†ïÌôïÎèÑ Î∞è ÏùëÎãµ ÏãúÍ∞ÑÏùÑ ÌÖåÏä§Ìä∏Ìï©ÎãàÎã§.

Ïã§Ìñâ Î∞©Î≤ï:
    python scripts/test_rag_performance.py
"""

import sys
import time
from pathlib import Path
from typing import List, Dict, Any
import logging

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.core.services.rag_service import get_rag_service

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class RAGPerformanceTester:
    """RAG ÏÑ±Îä• ÌÖåÏä§Ìä∏ ÌÅ¥ÎûòÏä§"""

    def __init__(self):
        """Initialize RAG Performance Tester"""
        self.rag_service = get_rag_service()

        # Test queries
        self.test_queries = [
            {
                "query": "Azure OpenAI API Rate Limit Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏùÑ Îïå Ïñ¥ÎñªÍ≤å Ìï¥Í≤∞ÌïòÎÇòÏöî?",
                "expected_collection": "incidents",
                "expected_keywords": ["rate limit", "429", "azure openai"]
            },
            {
                "query": "PostgreSQL Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§Ïóê Ïó∞Í≤∞Ìï† Ïàò ÏóÜÏùÑ Îïå ÌôïÏù∏Ìï¥Ïïº Ìï† ÏÇ¨Ìï≠ÏùÄ?",
                "expected_collection": "manuals",
                "expected_keywords": ["postgresql", "connection", "pg_hba.conf"]
            },
            {
                "query": "tasks ÌÖåÏù¥Î∏îÏùò Íµ¨Ï°∞ÏôÄ Ï£ºÏöî Ïª¨ÎüºÏóê ÎåÄÌï¥ ÏÑ§Î™ÖÌï¥Ï£ºÏÑ∏Ïöî.",
                "expected_collection": "schemas",
                "expected_keywords": ["tasks", "agent_name", "status"]
            },
            {
                "query": "AI/ML Í¥ÄÎ†® Î¨∏Ï†úÍ∞Ä ÏÉùÍ≤ºÏùÑ Îïå ÎàÑÍµ¨ÏóêÍ≤å Ïó∞ÎùΩÌï¥Ïïº ÌïòÎÇòÏöî?",
                "expected_collection": "contacts",
                "expected_keywords": ["ÍπÄÏ≤†Ïàò", "AI/ML", "chulsoo.kim"]
            },
            {
                "query": "Qdrant Vector DatabaseÏùò Î©îÎ™®Î¶¨ ÏÇ¨Ïö©ÎüâÏùÑ Ï§ÑÏù¥Îäî Î∞©Î≤ïÏùÄ?",
                "expected_collection": "incidents",
                "expected_keywords": ["qdrant", "memory", "quantization"]
            },
            {
                "query": "Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Î∞±ÏóÖÏùÑ Ïñ¥ÎñªÍ≤å ÏàòÌñâÌïòÎÇòÏöî?",
                "expected_collection": "manuals",
                "expected_keywords": ["backup", "pg_dump", "postgresql"]
            },
            {
                "query": "agent_executions ÌÖåÏù¥Î∏îÏóêÏÑú AgentÎ≥Ñ ÏÑ±Îä• ÌÜµÍ≥ÑÎ•º Ï°∞ÌöåÌïòÎ†§Î©¥?",
                "expected_collection": "schemas",
                "expected_keywords": ["agent_executions", "performance", "avg"]
            },
            {
                "query": "DevOps Îã¥ÎãπÏûêÏùò Ïó∞ÎùΩÏ≤òÎ•º ÏïåÎ†§Ï£ºÏÑ∏Ïöî.",
                "expected_collection": "contacts",
                "expected_keywords": ["Î∞ïÏßÄÌõà", "DevOps", "infrastructure"]
            },
            {
                "query": "RAG Í≤ÄÏÉâ Ï†ïÌôïÎèÑÍ∞Ä ÎÇÆÏùÑ ÎïåÏùò Ìï¥Í≤∞ Î∞©Î≤ïÏùÄ?",
                "expected_collection": "incidents",
                "expected_keywords": ["rag", "search", "accuracy", "threshold"]
            },
            {
                "query": "QdrantÏóêÏÑú Ïª¨Î†âÏÖòÏùÑ ÏÉùÏÑ±ÌïòÎäî Î∞©Î≤ïÏùÑ ÏïåÎ†§Ï£ºÏÑ∏Ïöî.",
                "expected_collection": "manuals",
                "expected_keywords": ["qdrant", "collection", "create"]
            }
        ]

        # Results
        self.results = []

    def test_semantic_search(
        self,
        query: str,
        collection_type: str,
        score_threshold: float = 0.7
    ) -> Dict[str, Any]:
        """
        ÏùòÎØ∏ Í≤ÄÏÉâ ÌÖåÏä§Ìä∏

        Args:
            query: Í≤ÄÏÉâ ÏøºÎ¶¨
            collection_type: Ïª¨Î†âÏÖò ÌÉÄÏûÖ
            score_threshold: ÏµúÏÜå Ïú†ÏÇ¨ÎèÑ Ï†êÏàò

        Returns:
            Dict: ÌÖåÏä§Ìä∏ Í≤∞Í≥º
        """
        start_time = time.time()

        try:
            results = self.rag_service.semantic_search(
                collection_type=collection_type,
                query=query,
                limit=5,
                score_threshold=score_threshold
            )

            response_time = time.time() - start_time

            return {
                "success": True,
                "response_time": response_time,
                "num_results": len(results),
                "top_score": results[0]["score"] if results else 0,
                "results": results
            }

        except Exception as e:
            response_time = time.time() - start_time
            return {
                "success": False,
                "response_time": response_time,
                "error": str(e)
            }

    def test_rag_query(
        self,
        query: str,
        collection_types: List[str]
    ) -> Dict[str, Any]:
        """
        RAG ÏøºÎ¶¨ ÌÖåÏä§Ìä∏

        Args:
            query: ÏÇ¨Ïö©Ïûê ÏßàÎ¨∏
            collection_types: Í≤ÄÏÉâÌï† Ïª¨Î†âÏÖò ÌÉÄÏûÖ

        Returns:
            Dict: ÌÖåÏä§Ìä∏ Í≤∞Í≥º
        """
        start_time = time.time()

        try:
            result = self.rag_service.rag_query(
                query=query,
                collection_types=collection_types,
                max_context_results=3
            )

            response_time = time.time() - start_time

            return {
                "success": True,
                "response_time": response_time,
                "answer_length": len(result["answer"]),
                "tokens_used": result["usage"]["total_tokens"],
                "answer": result["answer"][:200] + "..." if len(result["answer"]) > 200 else result["answer"]
            }

        except Exception as e:
            response_time = time.time() - start_time
            return {
                "success": False,
                "response_time": response_time,
                "error": str(e)
            }

    def evaluate_relevance(
        self,
        query_data: Dict[str, Any],
        search_results: List[Dict]
    ) -> float:
        """
        Í≤ÄÏÉâ Í≤∞Í≥ºÏùò Í¥ÄÎ†®ÏÑ± ÌèâÍ∞Ä

        Args:
            query_data: ÏøºÎ¶¨ Îç∞Ïù¥ÌÑ∞ (expected_keywords Ìè¨Ìï®)
            search_results: Í≤ÄÏÉâ Í≤∞Í≥º

        Returns:
            float: Í¥ÄÎ†®ÏÑ± Ï†êÏàò (0-1)
        """
        if not search_results:
            return 0.0

        expected_keywords = query_data["expected_keywords"]
        top_result = search_results[0]
        content = top_result["payload"].get("content", "").lower()

        # Check how many expected keywords are found
        found_keywords = sum(
            1 for keyword in expected_keywords
            if keyword.lower() in content
        )

        relevance_score = found_keywords / len(expected_keywords)

        return relevance_score

    def run_tests(self):
        """Î™®Îì† ÌÖåÏä§Ìä∏ Ïã§Ìñâ"""
        logger.info("="*60)
        logger.info("RAG Performance Test")
        logger.info("="*60)

        for i, test_case in enumerate(self.test_queries, 1):
            logger.info(f"\n--- Test {i}/{len(self.test_queries)} ---")
            logger.info(f"Query: {test_case['query']}")
            logger.info(f"Expected Collection: {test_case['expected_collection']}")

            # Test semantic search
            search_result = self.test_semantic_search(
                query=test_case["query"],
                collection_type=test_case["expected_collection"],
                score_threshold=0.7
            )

            if search_result["success"]:
                logger.info(f"‚úì Search Response Time: {search_result['response_time']:.3f}s")
                logger.info(f"‚úì Results Found: {search_result['num_results']}")
                logger.info(f"‚úì Top Score: {search_result['top_score']:.3f}")

                # Evaluate relevance
                relevance = self.evaluate_relevance(test_case, search_result["results"])
                logger.info(f"‚úì Relevance Score: {relevance:.3f}")

                result = {
                    "test_id": i,
                    "query": test_case["query"],
                    "collection": test_case["expected_collection"],
                    "search_success": True,
                    "search_response_time": search_result["response_time"],
                    "num_results": search_result["num_results"],
                    "top_score": search_result["top_score"],
                    "relevance_score": relevance
                }
            else:
                logger.error(f"‚úó Search Failed: {search_result.get('error')}")
                result = {
                    "test_id": i,
                    "query": test_case["query"],
                    "collection": test_case["expected_collection"],
                    "search_success": False,
                    "error": search_result.get("error")
                }

            self.results.append(result)

        # Print summary
        self.print_summary()

    def print_summary(self):
        """ÌÖåÏä§Ìä∏ Í≤∞Í≥º ÏöîÏïΩ Ï∂úÎ†•"""
        logger.info("\n" + "="*60)
        logger.info("Test Summary")
        logger.info("="*60)

        total_tests = len(self.results)
        successful_tests = sum(1 for r in self.results if r.get("search_success"))

        # Calculate averages
        avg_response_time = sum(
            r.get("search_response_time", 0)
            for r in self.results if r.get("search_success")
        ) / successful_tests if successful_tests > 0 else 0

        avg_top_score = sum(
            r.get("top_score", 0)
            for r in self.results if r.get("search_success")
        ) / successful_tests if successful_tests > 0 else 0

        avg_relevance = sum(
            r.get("relevance_score", 0)
            for r in self.results if r.get("search_success")
        ) / successful_tests if successful_tests > 0 else 0

        logger.info(f"Total Tests: {total_tests}")
        logger.info(f"Successful Tests: {successful_tests}")
        logger.info(f"Failed Tests: {total_tests - successful_tests}")
        logger.info(f"Success Rate: {(successful_tests/total_tests*100):.1f}%")
        logger.info(f"\nAverage Response Time: {avg_response_time:.3f}s")
        logger.info(f"Average Top Score: {avg_top_score:.3f}")
        logger.info(f"Average Relevance Score: {avg_relevance:.3f}")

        # Evaluation criteria
        logger.info("\n" + "="*60)
        logger.info("Evaluation Criteria")
        logger.info("="*60)

        criteria = {
            "Response Time < 3s": avg_response_time < 3.0,
            "Top Score > 0.8": avg_top_score > 0.8,
            "Relevance Score > 0.7": avg_relevance > 0.7,
            "Success Rate > 90%": (successful_tests/total_tests) > 0.9
        }

        for criterion, passed in criteria.items():
            status = "‚úì PASS" if passed else "‚úó FAIL"
            logger.info(f"{status}: {criterion}")

        # Overall result
        all_passed = all(criteria.values())
        logger.info("\n" + "="*60)
        if all_passed:
            logger.info("üéâ Overall Result: PASS")
        else:
            logger.info("‚ö†Ô∏è  Overall Result: FAIL")
        logger.info("="*60)

        return all_passed


def main():
    """Main function"""
    tester = RAGPerformanceTester()
    tester.run_tests()


if __name__ == "__main__":
    main()
